{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from causalimpact import CausalImpact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_colours = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "period_in_days = 7\n",
    "fcn_compare = lambda a,b: abs(a-b)/max(a,b)\n",
    "\n",
    "\n",
    "# Season-Trend decomposition using LOESS.\n",
    "def decompose_signal(input_signal:pd.Series, period_in_days=14, minimum_heartbeat=0.85) -> pd.DataFrame:\n",
    "    sales_decomposition_LOESS = STL(input_signal, period=period_in_days).fit()\n",
    "    seasonality_flag = sales_decomposition_LOESS.trend > minimum_heartbeat\n",
    "    df = pd.DataFrame({\n",
    "        'heartbeat_flag': seasonality_flag,\n",
    "        'trend': sales_decomposition_LOESS.trend,\n",
    "        'seasonal': sales_decomposition_LOESS.seasonal,\n",
    "        'residual': sales_decomposition_LOESS.resid\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "def compare_promo_regular_sales(sales:pd.Series, promo:pd.Series, inferred_availability:pd.Series, idx_holiday_to_exclude:pd.Series, min_promo_days=3, min_regular_days=6) -> dict:\n",
    "    '''\n",
    "        Explain this well as it will go on the paper...\n",
    "        \n",
    "        This method splits a CFAV SKU into promotional and regular chunks taking into account the inferred availability (using LOESS)\\\n",
    "        and the holiday periods (any other event can be included in that idx).\n",
    "        \n",
    "        The method divides the selling days into sequences of regular and normal days and calculates marginal sales.\n",
    "    \n",
    "        'avg_promo_sales' and 'avg_regular_sales' are the sales aggregated across all the slots, whereas\n",
    "        'slot_promo_avg_sales' and 'slot_promo_avg_sales' represent each slot.\n",
    "        \n",
    "        The total sales and total days are not returned, will I need them?\n",
    "        \n",
    "        min_promo_days=3, min_regular_days=6 decide the minimum number of days to be taken into consideration for the sequences.\n",
    "        \n",
    "            TO-DO: Add the beginning and end of the promotional periods\n",
    "        \n",
    "        Updates:\n",
    "        25.10.2020 - First attempt\n",
    "    \n",
    "    '''\n",
    "\n",
    "    analysis_results = []\n",
    "    \n",
    "    # only if there are promos\n",
    "    if promo.sum() > 0:\n",
    "\n",
    "        availability_sku_A = inferred_availability & (~idx_holiday_to_exclude)\n",
    "        availability_value_sku_A = availability_sku_A.sum()/len(availability_sku_A)\n",
    "\n",
    "        # Split the promotions into slots\n",
    "        idx_pre_intervention, idx_post_intervention = split_promos_into_sequences(promo, min_promo_days=min_promo_days, min_regular_days=min_regular_days)\n",
    "\n",
    "        num_promo_slots = len(idx_pre_intervention)\n",
    "\n",
    "        slot_promo_sales = np.zeros(num_promo_slots)\n",
    "        slot_regular_sales = np.zeros(num_promo_slots)\n",
    "\n",
    "        slot_promo_days = np.zeros(num_promo_slots)\n",
    "        slot_regular_days = np.zeros(num_promo_slots)\n",
    "\n",
    "        for idx_promo_slot in range(0, num_promo_slots):\n",
    "            idx_pre_intervention_current = idx_pre_intervention[idx_promo_slot]\n",
    "            idx_post_intervention_current = idx_post_intervention[idx_promo_slot]\n",
    "\n",
    "            slot_promo_sales[idx_promo_slot] = sales[idx_post_intervention_current].sum()\n",
    "            slot_promo_days[idx_promo_slot] = idx_post_intervention_current.sum()\n",
    "\n",
    "            slot_regular_sales[idx_promo_slot] = sales[idx_pre_intervention_current].sum()\n",
    "            slot_regular_days[idx_promo_slot] = idx_pre_intervention_current.sum()\n",
    "\n",
    "        slot_promo_avg_sales = np.divide(slot_promo_sales, slot_promo_days)\n",
    "        slot_regular_avg_sales = np.divide(slot_regular_sales, slot_regular_days)\n",
    "        # totals\n",
    "        total_slot_promo_days = slot_promo_days.sum()\n",
    "        if total_slot_promo_days>0:\n",
    "            avg_promo_sales = slot_promo_sales.sum()/total_slot_promo_days\n",
    "        else:\n",
    "            avg_promo_sales = 0\n",
    "        \n",
    "        total_slot_regular_days = slot_regular_days.sum()\n",
    "        if total_slot_regular_days>0:\n",
    "            avg_regular_sales = slot_regular_sales.sum()/total_slot_regular_days\n",
    "        else:\n",
    "            avg_regular_sales = 0\n",
    "            \n",
    "\n",
    "        # difference between the averages during promo and regular\n",
    "        difference_averages_promo_to_regular = avg_promo_sales-avg_regular_sales\n",
    "        # cumulative difference\n",
    "        cum_difference_sales_promo_to_regular = slot_promo_sales.sum()-slot_regular_sales.sum()\n",
    "        \n",
    "        analysis_results.append({\n",
    "            'num_promo_slots': num_promo_slots,\n",
    "            'avg_promo_sales': avg_promo_sales,\n",
    "            'avg_regular_sales': avg_regular_sales,\n",
    "            'promo_days': total_slot_promo_days, \n",
    "            'regular_days':total_slot_regular_days,\n",
    "            'difference_averages_promo_to_regular': difference_averages_promo_to_regular,\n",
    "            'cum_difference_sales_promo_to_regular': cum_difference_sales_promo_to_regular,\n",
    "            'slot_promo_avg_sales': slot_promo_avg_sales,\n",
    "            'slot_regular_avg_sales': slot_regular_avg_sales,\n",
    "            'availability_value_sku_A': availability_value_sku_A\n",
    "        })\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "def holiday_to_exclude(df_summary:pd.DataFrame, holidays):\n",
    "    idx_holidays  = df_summary.index.isin(holidays)\n",
    "    idx_covid19 = (df_summary.index >= 20210701) & (df_summary.index <= 20211001)\n",
    "    return idx_holidays | idx_covid19\n",
    "\n",
    "def customers_by_date_shop_pairs(customers_history_AB:'pd.DataFrame', period_in_days=7):\n",
    "    df = customers_history_AB\\\n",
    "        .groupby(['date_key','shop_code'], as_index=False)\\\n",
    "        .agg(\n",
    "            customers=('customer_key','nunique')\n",
    "        )\\\n",
    "        .pivot(index='date_key',columns='shop_code',values='customers')\\\n",
    "        .fillna(0)\\\n",
    "        .applymap(int)\n",
    "    return df\n",
    "\n",
    "def customers_cross_from_victim_shop(customers_history_AB:'pd.DataFrame'):\n",
    "\n",
    "    shop_code_A, shop_code_B = customers_history_AB.shop_code.unique()\n",
    "\n",
    "    df = customers_history_AB\\\n",
    "        .assign(\n",
    "            is_cross_customer = lambda s: (s.date_key > s.groupby('customer_key')['date_key'].transform('min')) & (s.date_key == s.groupby(['customer_key','shop_key'])['date_key'].transform('min'))\n",
    "        )\\\n",
    "        .query(f\"is_cross_customer\")\\\n",
    "        .groupby(['date_key','shop_code'], as_index=False)\\\n",
    "        .agg(\n",
    "            cross_customers=('customer_key','nunique')\n",
    "        )\\\n",
    "        .pivot(index='date_key', columns='shop_code', values='cross_customers')\\\n",
    "        .rename(columns={shop_code_A:f'{shop_code_A}_cross',shop_code_B:f'{shop_code_B}_cross'})\\\n",
    "        .fillna(0)\\\n",
    "        .applymap(int)\n",
    "    \n",
    "    if f'{shop_code_A}_cross' not in df.columns:\n",
    "        df[f'{shop_code_A}_cross'] = 0\n",
    "    \n",
    "    if f'{shop_code_B}_cross' not in df.columns:\n",
    "        df[f'{shop_code_B}_cross'] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "# def customers_component_AB(decompostition):\n",
    "#     shop_code_A, shop_code_B = [col for col in customers_by_date_shop_pairs.columns if isinstance(col, int)]\n",
    "#     customers_shop_A = customers_by_date_shop_pairs[shop_code_A]\n",
    "#     df_decomposition_A = decompose_signal()\n",
    "\n",
    "\n",
    "\n",
    "# def compare_cross_regular_customers(df_store:'pd.DataFrame', df_history:'pd.DataFrame', df_components:'pd.DataFrame', shop_code_A, shop_code_B, min_date_key, holidays, min_cross_days=5, min_regular_days=10):\n",
    "\n",
    "#     df_cross = df_history\\\n",
    "#         .query(f'shop_code.isin({[shop_code_A,shop_code_B]})')\\\n",
    "#         .assign(\n",
    "#             is_cross_customer = lambda s: (s.date_key > s.groupby('customer_key')['date_key'].transform('min')) & (s.date_key == s.groupby(['customer_key','shop_key'])['date_key'].transform('min'))\n",
    "#         )\\\n",
    "#         .query(f\"is_cross_customer\")\\\n",
    "#         .groupby(['date_key','shop_code'], as_index=False)\\\n",
    "#         .agg(\n",
    "#             cross_customers=('customer_key','nunique')\n",
    "#         )\\\n",
    "#         .pivot(index='date_key', columns='shop_code', values='cross_customers')\\\n",
    "#         .reset_index()\\\n",
    "#         .rename(columns={shop_code_A:f'{shop_code_A}_cross',shop_code_B:f'{shop_code_B}_cross'})\\\n",
    "#         .fillna(0)\\\n",
    "#         .applymap(int)\n",
    "    \n",
    "#     df_summary = df_store[['date_key',shop_code_A,shop_code_B]]\\\n",
    "#         .merge(df_cross, on='date_key', how='left')\\\n",
    "#         .fillna(0)\\\n",
    "#         .applymap(int)\\\n",
    "#         .query(f'date_key >= {min_date_key}')\n",
    "#     df_summary[[f'{shop_code_A}_iscross',f'{shop_code_B}_iscross']] = df_summary[[f'{shop_code_A}_cross',f'{shop_code_B}_cross']].applymap(lambda x: False if x==0 else True)\n",
    "\n",
    "#     customers_shop_A = df_summary[shop_code_A]\n",
    "#     cross_shop_A = df_summary[f'{shop_code_A}_iscross']\n",
    "#     inferred_availability_shop_A = df_components.query(f'date_key >= {min_date_key}')[f'{shop_code_A}_heartbeat_flag']\n",
    "    \n",
    "#     analysis_results = []\n",
    "#     idx_holiday_to_exclude = holiday_to_exclude(df_summary, holidays)\n",
    "    \n",
    "#     # only if there are cross shops\n",
    "#     if cross_shop_A.sum() > 0:\n",
    "\n",
    "#         availability_shop_A = inferred_availability_shop_A & (~idx_holiday_to_exclude)\n",
    "#         availability_value_shop_A = availability_shop_A.sum()/len(availability_shop_A)\n",
    "\n",
    "#         # Split the cross shops into slots\n",
    "#         idx_pre_intervention, idx_post_intervention = split_cross_into_sequences(cross_shop_A, \\\n",
    "#             min_cross_days=min_cross_days, min_regular_days=min_regular_days)\n",
    "\n",
    "#         num_cross_slots = len(idx_pre_intervention)\n",
    "\n",
    "#         slot_cross_customers = np.zeros(num_cross_slots)\n",
    "#         slot_regular_customers = np.zeros(num_cross_slots)\n",
    "\n",
    "#         slot_cross_days = np.zeros(num_cross_slots)\n",
    "#         slot_regular_days = np.zeros(num_cross_slots)\n",
    "\n",
    "#         for idx_cross_slot in range(0, num_cross_slots):\n",
    "#             idx_pre_intervention_current = idx_pre_intervention[idx_cross_slot]\n",
    "#             idx_post_intervention_current = idx_post_intervention[idx_cross_slot]\n",
    "\n",
    "#             slot_cross_customers[idx_cross_slot] = customers_shop_A[idx_post_intervention_current].sum()\n",
    "#             slot_cross_days[idx_cross_slot] = idx_post_intervention_current.sum()\n",
    "\n",
    "#             slot_regular_customers[idx_cross_slot] = customers_shop_A[idx_pre_intervention_current].sum()\n",
    "#             slot_regular_days[idx_cross_slot] = idx_pre_intervention_current.sum()\n",
    "\n",
    "#         slot_cross_avg_customers = np.divide(slot_cross_customers, slot_cross_days)\n",
    "#         slot_regular_avg_customers = np.divide(slot_regular_customers, slot_regular_days)\n",
    "#         # totals\n",
    "#         total_slot_cross_days = slot_cross_days.sum()\n",
    "#         if total_slot_cross_days>0:\n",
    "#             avg_cross_customers = slot_cross_customers.sum()/total_slot_cross_days\n",
    "#         else:\n",
    "#             avg_cross_customers = 0\n",
    "        \n",
    "#         total_slot_regular_days = slot_regular_days.sum()\n",
    "#         if total_slot_regular_days>0:\n",
    "#             avg_regular_customers = slot_regular_customers.sum()/total_slot_regular_days\n",
    "#         else:\n",
    "#             avg_regular_customers = 0\n",
    "            \n",
    "\n",
    "#         # difference between the averages during cross shops and regular\n",
    "#         difference_averages_cross_to_regular = avg_cross_customers-avg_regular_customers\n",
    "#         # cumulative difference\n",
    "#         cum_difference_customers_cross_to_regular = slot_cross_customers.sum()-slot_regular_customers.sum()\n",
    "        \n",
    "#         analysis_results.append({\n",
    "#             'shop_code_A':shop_code_A,\n",
    "#             'shop_code_B':shop_code_B,\n",
    "#             'num_cross_slots': num_cross_slots,\n",
    "#             'avg_cross_customers': avg_cross_customers,\n",
    "#             'avg_regular_customers': avg_regular_customers,\n",
    "#             'cross_days': total_slot_cross_days, \n",
    "#             'regular_days':total_slot_regular_days,\n",
    "#             'difference_averages_cross_to_regular': difference_averages_cross_to_regular,\n",
    "#             'cum_difference_customers_cross_to_regular': cum_difference_customers_cross_to_regular,\n",
    "#             'slot_promo_avg_customers': slot_cross_avg_customers,\n",
    "#             'slot_regular_avg_customers': slot_regular_avg_customers,\n",
    "#             'availability_value_shop_A': availability_value_shop_A\n",
    "#         })\n",
    "    \n",
    "#     return analysis_results\n",
    "\n",
    "\n",
    "def calculate_causal_impact_with_covariates(\n",
    "        promo_sku_A:pd.Series, \n",
    "        availability_sku_A:pd.Series,\n",
    "        sales_sku_B:pd.Series,\n",
    "        promo_sku_B:pd.Series, \n",
    "        availability_sku_B:pd.Series,\n",
    "        idx_pre_intervention:pd.Series, \n",
    "        idx_post_intervention:pd.Series,\n",
    "        idx_holiday_to_exclude:pd.Series,\n",
    "        min_diff_in_units_from_reg_to_promo, \n",
    "        min_ratio_change = 0.3,\n",
    "        do_exclude_promos_SKU_B = True, \n",
    "        be_verbose=True,\n",
    "        min_overlapping_days_regular=10,\n",
    "        min_overlapping_days_promo=5\n",
    "    ):\n",
    "    '''\n",
    "        This is the method used to populate the paper results\n",
    "    '''\n",
    "\n",
    "    # use this flag to exclude sku_B if on promo\n",
    "    total_days = promo_sku_A.shape[0]\n",
    "    num_days = promo_sku_A.index\n",
    "    combined_availability = availability_sku_A & availability_sku_B & (~idx_holiday_to_exclude)\n",
    "\n",
    "    causal_analysis = []\n",
    "\n",
    "    # sales_sku_B = df_sales_covariates.iloc[:,0]\n",
    "\n",
    "    total_slots = len(idx_pre_intervention)\n",
    "\n",
    "    for idx_promo_slot in range(0, total_slots):\n",
    "\n",
    "        idx_pre_intervention_current = idx_pre_intervention[idx_promo_slot]\n",
    "        idx_post_intervention_current = idx_post_intervention[idx_promo_slot]\n",
    "\n",
    "        # # #\n",
    "        # promo days == 'post-intervention'\n",
    "        # # #\n",
    "        idx_overlapping_days_promo = combined_availability & idx_post_intervention_current\n",
    "        total_overlapping_days_promo = idx_overlapping_days_promo.sum()\n",
    "\n",
    "\n",
    "        # overlapping promo days. Both SKUs on promo, \"competing promos\"\n",
    "        idx_competing_promo_days = idx_overlapping_days_promo & promo_sku_B\n",
    "        competing_promo_days = idx_competing_promo_days.sum()\n",
    "\n",
    "\n",
    "        # # #\n",
    "        # regular days == 'pre-intervention'\n",
    "        # # #\n",
    "        # A period should not be marked as 'regular' if SKU_B is on promotion\n",
    "        if do_exclude_promos_SKU_B:\n",
    "            idx_overlapping_days_regular = combined_availability & idx_pre_intervention_current & (~promo_sku_B)\n",
    "        else:\n",
    "            idx_overlapping_days_regular = combined_availability & idx_pre_intervention_current\n",
    "        total_overlapping_days_regular = idx_overlapping_days_regular.sum()\n",
    "        \n",
    "        # Minimum requirements of overlap. Otherwise the analysis does not make much sense.\n",
    "        # numerical index (for Causal Impact)\n",
    "        if (total_overlapping_days_regular>=min_overlapping_days_regular) & (total_overlapping_days_promo>=min_overlapping_days_promo):\n",
    "            \n",
    "            ind_regular_days = num_days[idx_overlapping_days_regular]\n",
    "            start_regular = ind_regular_days.min()\n",
    "            end_regular = ind_regular_days.max()\n",
    "            \n",
    "            ind_promo_days = num_days[idx_overlapping_days_promo]\n",
    "            start_promo = ind_promo_days.min()\n",
    "            end_promo = ind_promo_days.max()\n",
    "            gap_days = (start_promo - end_regular).days - 1\n",
    "\n",
    "            # sales of SKU_B\n",
    "            # during regular\n",
    "            sku_B_regular_avg_sales = sales_sku_B[idx_overlapping_days_regular].mean()\n",
    "            # during promo\n",
    "            sku_B_avg_sales_during_promo_sku_A = sales_sku_B[idx_overlapping_days_promo].mean()\n",
    "            # Difference in average sales between the regular and the promotional one.\n",
    "            diff_in_units_from_reg_to_promo = sku_B_regular_avg_sales-sku_B_avg_sales_during_promo_sku_A\n",
    "            \n",
    "            # can we review the post promotional sales?\n",
    "            end_promo_loc = num_days.get_loc(end_promo)\n",
    "            post_period_start_loc = end_promo_loc+1\n",
    "            post_period_end_loc = min(end_promo_loc+1+min_overlapping_days_regular, total_days)\n",
    "            sku_B_regular_post_promo = sales_sku_B.iloc[post_period_start_loc:post_period_end_loc]\n",
    "            post_promo_days = sku_B_regular_post_promo.shape[0]\n",
    "            sku_B_regular_post_promo_avg_sales = sku_B_regular_post_promo.mean()\n",
    "            # post promo should be larger than during the cannibalisation\n",
    "            diff_in_units_from_promo_to_pos_promo = sku_B_avg_sales_during_promo_sku_A-sku_B_regular_post_promo_avg_sales\n",
    "            \n",
    "            post_promo_flag = (-diff_in_units_from_promo_to_pos_promo > min_diff_in_units_from_reg_to_promo*0.25)\n",
    "            '''\n",
    "            if idx_promo_slot+1 < total_slots:\n",
    "                # during regular\n",
    "                idx_reg_post_intervention = idx_pre_intervention[idx_promo_slot+1]\n",
    "                sku_B_regular_post_promo_avg_sales = sales_sku_B[idx_reg_post_intervention].mean()\n",
    "                # post promo should be larger than during the cannibalisation\n",
    "                diff_in_units_from_promo_to_pos_promo = sku_B_avg_sales_during_promo_sku_A-sku_B_regular_post_promo_avg_sales\n",
    "                post_promo_flag = (-diff_in_units_from_promo_to_pos_promo>min_diff_in_units_from_reg_to_promo)\n",
    "            else:\n",
    "                diff_in_units_from_promo_to_pos_promo = np.nan\n",
    "                post_promo_flag = True\n",
    "            '''\n",
    "\n",
    "            # delta\n",
    "            ratio_change = fcn_compare(sku_B_avg_sales_during_promo_sku_A, sku_B_regular_avg_sales)\n",
    "            if be_verbose:\n",
    "                print(f'Summary of the current scenario (slot {idx_promo_slot})')\n",
    "                print(f'Before SKU A going on promo, the SKUs overlap for {total_overlapping_days_regular} days (promos on sku_B excluded: {do_exclude_promos_SKU_B})')\n",
    "                \n",
    "                if gap_days > 0:\n",
    "                    print(f'There is a gap of {gap_days} days between the regular days and the beginning of the promotion (due to availability/promotional period of SKU_B)')\n",
    "                print(f'When SKU_A is on promo, the SKUs overlap for {total_overlapping_days_promo} days')\n",
    "                print(f'During the overlapping days, SKU B is on promo for {competing_promo_days} days')\n",
    "\n",
    "                print(f'Average sales of sku B before sku A on promo {sku_B_regular_avg_sales:.2f}')\n",
    "                print(f'Average sales of sku B during sku A on promo {sku_B_avg_sales_during_promo_sku_A:.2f}')\n",
    "                print(f'Average sales of sku B after sku A on promo {sku_B_regular_post_promo_avg_sales:.2f} over {post_promo_days} days - {post_promo_flag}')\n",
    "                print(f'Diff in units from regular to promotion {-diff_in_units_from_reg_to_promo:.2f}')\n",
    "                print(f'Diff in units from cannibalisation to regular {-diff_in_units_from_promo_to_pos_promo:.2f}')\n",
    "                \n",
    "                print(f'Ratio of change {ratio_change:3.2f} (the lower the closer (0,1)) {ratio_change>min_ratio_change} \\n')\n",
    "\n",
    "\n",
    "            if (ratio_change>min_ratio_change) & (diff_in_units_from_reg_to_promo > min_diff_in_units_from_reg_to_promo) & post_promo_flag:\n",
    "\n",
    "                idx_regular_days = np.array([start_regular, end_regular]).tolist()\n",
    "                idx_promo_days   = np.array([start_promo, end_promo]).tolist()\n",
    "\n",
    "                print('Running Causal Impact...')\n",
    "                ci = CausalImpact(sales_sku_B, idx_regular_days, idx_promo_days)\n",
    "                '''\n",
    "                https://github.com/dafiti/causalimpact/blob/8d881fc5c270348d8c8ff59c936997a75d7c5fac/causalimpact/main.py#L88\n",
    "                \n",
    "                First column must contain the `y` measured value \n",
    "                while the others contain the covariates `X` that are used in the \n",
    "                linear regression component of the model.\n",
    "                '''\n",
    "                #ci.lower_upper_percentile\n",
    "                avg_actual = ci.summary_data.loc['actual', 'average']\n",
    "                # Had the promo not been launched on the cannibal, we would have sold this amount.\n",
    "                avg_predicted = ci.summary_data.loc['predicted', 'average']\n",
    "                avg_abs_effect = ci.summary_data.loc['abs_effect', 'average']\n",
    "                # This can be seen as the number of units that the cannibal is taking from the victim\n",
    "                cum_abs_effect = ci.summary_data.loc['abs_effect', 'cumulative']\n",
    "                posterior_tail_prob = ci.p_value\n",
    "                prob_causal_effect = (1-ci.p_value)*100\n",
    "                print(f'CausalImpact >> Probability of a causal event {prob_causal_effect:.2f}')\n",
    "\n",
    "                temp_dict = {\n",
    "                    'slot_number': idx_promo_slot,\n",
    "                    'idx_regular_days': idx_regular_days,\n",
    "                    'idx_promo_days': idx_promo_days,\n",
    "                    'total_overlapping_days_regular': total_overlapping_days_regular,\n",
    "                    'regular_to_promo_gap': gap_days,\n",
    "                    'total_overlapping_days_promo': total_overlapping_days_promo,\n",
    "                    'competing_promo_days': competing_promo_days,\n",
    "                    'sku_B_regular_avg_sales': sku_B_regular_avg_sales,\n",
    "                    'sku_B_avg_sales_during_promo_sku_A': sku_B_avg_sales_during_promo_sku_A,\n",
    "                    'diff_in_units_from_reg_to_promo': diff_in_units_from_reg_to_promo,\n",
    "                    'diff_in_units_from_promo_to_pos_promo': diff_in_units_from_promo_to_pos_promo,\n",
    "                    'ratio_change': ratio_change,\n",
    "                    'avg_actual':avg_actual,\n",
    "                    'avg_predicted': avg_predicted,\n",
    "                    'avg_abs_effect': avg_abs_effect,\n",
    "                    'cum_abs_effect': cum_abs_effect,\n",
    "                    'posterior_tail_prob': posterior_tail_prob,\n",
    "                    'prob_causal_effect': prob_causal_effect\n",
    "                }\n",
    "                causal_analysis.append(temp_dict)\n",
    "                \n",
    "    return causal_analysis\n",
    "\n",
    "def add_graph_relationship(node_A, node_B, edge_properties: dict):\n",
    "    DG = nx.DiGraph()\n",
    "\n",
    "    DG.add_node(node_A['name'], **node_A['properties'])\n",
    "\n",
    "    d = dict()\n",
    "    DG.add_node(node_B['name'], **node_B['properties'])\n",
    "\n",
    "    edge_label = '\\n'.join([f'{k}: {v:3.2f}' for k,v in edge_properties.items()])  \n",
    "    DG.add_edge(node_A['name'], node_B['name'], **edge_properties, label=edge_label)\n",
    "\n",
    "        \n",
    "def split_promos_into_sequences(idx_promos:pd.Series, min_promo_days=5, min_regular_days=10) -> tuple:\n",
    "    '''\n",
    "    Group the indices of a promotion into sequences of pre and post promotion\n",
    "    '''\n",
    "    # Groups/sequences\n",
    "    seqs = (idx_promos.shift(1)!=idx_promos).cumsum()\n",
    "    promo_seqs = seqs[idx_promos]\n",
    "    # Indices\n",
    "    idx_pre_intervention = []\n",
    "    idx_post_intervention = []\n",
    "    for value_promo_seqs in promo_seqs.unique():\n",
    "        idx_current_promo = seqs==value_promo_seqs\n",
    "        prev_seq = value_promo_seqs-1\n",
    "        idx_current_regular = seqs==prev_seq\n",
    "        current_promo_length = idx_current_promo.sum()\n",
    "        current_regular_length = idx_current_regular.sum()\n",
    "        if (current_promo_length >= min_promo_days) and (current_regular_length >= min_regular_days):\n",
    "            idx_pre_intervention.append(idx_current_regular)\n",
    "            idx_post_intervention.append(idx_current_promo)\n",
    "    return idx_pre_intervention, idx_post_intervention\n",
    "\n",
    "def split_cross_into_sequences(idx_cross: 'pd.Series', min_cross_days=5, min_regular_days=10):\n",
    "    # Groups/sequences\n",
    "    seqs = (idx_cross.shift(1)!=idx_cross).cumsum()\n",
    "    promo_seqs = seqs[idx_cross]\n",
    "    # Indices\n",
    "    idx_pre_intervention = []\n",
    "    idx_post_intervention = []\n",
    "    for value_promo_seqs in promo_seqs.unique():\n",
    "        idx_current_promo = seqs==value_promo_seqs\n",
    "        prev_seq = value_promo_seqs-1\n",
    "        idx_current_regular = seqs==prev_seq\n",
    "        current_promo_length = idx_current_promo.sum()\n",
    "        current_regular_length = idx_current_regular.sum()\n",
    "        if (current_promo_length >= min_cross_days) and (current_regular_length >= min_regular_days):\n",
    "            idx_pre_intervention.append(idx_current_regular)\n",
    "            idx_post_intervention.append(idx_current_promo)\n",
    "    return idx_pre_intervention, idx_post_intervention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dimension data\n",
    "product_hierarchy = pd.read_csv('./data/product_hierarchy.csv')\n",
    "store_cities = pd.read_csv('./data/store_cities.csv')\n",
    "\n",
    "# read fact data\n",
    "sales_zip = ZipFile('./data/sales.csv.zip', 'r')\n",
    "sales_data = sales_zip.read('sales.csv')\n",
    "sales_bytes = io.BytesIO(sales_data)\n",
    "sales_bytes.seek(0)\n",
    "sales = pd.read_csv(sales_bytes, converters={'promo_bin_1':str,'promo_bin_2':str,'promo_discount_type_2':str})\n",
    "df = sales.query(\"sales != 0 and ~sales.isnull()\").reset_index(drop=True)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df['promo'] = df[['promo_bin_1','promo_bin_2']].sum(axis=1) != ''\n",
    "\n",
    "df_sales = df.reindex(columns=['product_id', 'store_id', 'date', 'revenue', 'promo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from statsmodels.tsa.arima_process import ArmaProcess\n",
    "# from causalimpact import CausalImpact\n",
    "\n",
    "\n",
    "# np.random.seed(12345)\n",
    "# ar = np.r_[1, 0.9]\n",
    "# ma = np.array([1])\n",
    "# arma_process = ArmaProcess(ar, ma)\n",
    "# X = 100 + arma_process.generate_sample(nsample=100)\n",
    "# y = 1.2 * X + np.random.normal(size=100)\n",
    "# y[70:] += 5\n",
    "\n",
    "# data = pd.DataFrame({'y': y, 'X': X}, columns=['y', 'X'])\n",
    "# pre_period = [0, 69]\n",
    "# post_period = [70, 99]\n",
    "\n",
    "# ci = CausalImpact(data, pre_period, post_period)\n",
    "# print(ci.summary())\n",
    "# print(ci.summary(output='report'))\n",
    "# ci.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>sales_days</th>\n",
       "      <th>from_date</th>\n",
       "      <th>to_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>S0084</td>\n",
       "      <td>1033</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>S0063</td>\n",
       "      <td>1033</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>S0052</td>\n",
       "      <td>1033</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>S0027</td>\n",
       "      <td>1033</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>S0090</td>\n",
       "      <td>1033</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>S0136</td>\n",
       "      <td>312</td>\n",
       "      <td>2017-07-14</td>\n",
       "      <td>2019-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S0017</td>\n",
       "      <td>304</td>\n",
       "      <td>2017-06-09</td>\n",
       "      <td>2019-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S0007</td>\n",
       "      <td>213</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>S0057</td>\n",
       "      <td>171</td>\n",
       "      <td>2019-05-10</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>S0047</td>\n",
       "      <td>168</td>\n",
       "      <td>2019-05-17</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    store_id  sales_days  from_date    to_date\n",
       "83     S0084        1033 2017-01-02 2019-10-31\n",
       "62     S0063        1033 2017-01-02 2019-10-31\n",
       "51     S0052        1033 2017-01-02 2019-10-31\n",
       "26     S0027        1033 2017-01-02 2019-10-31\n",
       "89     S0090        1033 2017-01-02 2019-10-31\n",
       "..       ...         ...        ...        ...\n",
       "135    S0136         312 2017-07-14 2019-09-08\n",
       "16     S0017         304 2017-06-09 2019-09-08\n",
       "6      S0007         213 2019-03-29 2019-10-31\n",
       "56     S0057         171 2019-05-10 2019-10-31\n",
       "46     S0047         168 2019-05-17 2019-10-31\n",
       "\n",
       "[144 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of sales days by store\n",
    "\n",
    "df_sales_daily = df_sales.groupby([\"store_id\",\"date\"], as_index=False).agg(total_revenue=(\"revenue\",\"sum\"))\n",
    "\n",
    "df_sales_daily.groupby(\"store_id\", as_index=False).agg(\n",
    "    sales_days=(\"date\",\"count\"),\n",
    "    from_date=(\"date\",\"min\"),\n",
    "    to_date=(\"date\",\"max\")\n",
    ").sort_values(\"sales_days\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it seems that the revenue of store S0084 dose not follow any trend like peaking during the weekends..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use store S0084 as a sample\n",
    "# df_sales_S0084 = df_sales_daily[(df_sales_daily[\"store_id\"]==\"S0084\")&(df_sales_daily[\"date\"].dt.year==2017)]\n",
    "# df_sales_S0084_daily = df_sales_S0084.set_index(\"date\")[\"total_revenue\"]\n",
    "\n",
    "# # Fig sizes\n",
    "# fig_h = 10/1.5\n",
    "# fig_w = 18/1.5\n",
    "\n",
    "# # store_alias = 744\n",
    "# f, ax = plt.subplots(1,1,figsize=(fig_w*1.5*1.5, fig_h*1.5))\n",
    "# font_size = 13\n",
    "# do_save_decomposition = True\n",
    "\n",
    "# plt.rcParams.update({'font.size': font_size})\n",
    "# def_colours = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# x_axis = df_sales_S0084_daily.index\n",
    "# period_in_days=7\n",
    "\n",
    "\n",
    "\n",
    "# decomposition_results = decompose_signal(df_sales_S0084_daily, period_in_days=period_in_days, minimum_heartbeat=0.85)\n",
    "# df_decomposition = pd.DataFrame(decomposition_results)\n",
    "\n",
    "# idx_store = 0\n",
    "\n",
    "\n",
    "# idx_axis = 0\n",
    "# '''ax.plot(x_axis, df_store.total_units, label=f'{dept_id} sales {store_alias}', \n",
    "#         color=def_colours[idx_store], linewidth=2, alpha=0.75)'''\n",
    "\n",
    "# ax.plot(x_axis, df_sales_S0084_daily, label=f'Total revenue', \n",
    "#         color=def_colours[idx_store], linewidth=1+1, alpha=0.75)\n",
    "\n",
    "# ax.plot(x_axis, df_decomposition['trend'], label=f'Trend', \n",
    "#         color=def_colours[idx_store+1], linewidth=1.5+1, alpha=0.95)\n",
    "\n",
    "# ax.plot(x_axis, df_decomposition['seasonal'], label=f'Seasonal ({period_in_days} days)',\n",
    "#         color=def_colours[idx_store+2], linewidth=1.5+1, alpha=0.95)\n",
    "\n",
    "# ax.plot(x_axis, df_decomposition['residual'], label=f'Residual',\n",
    "#         color=def_colours[idx_store+3], linewidth=0.5+1, alpha=0.65)\n",
    "\n",
    "\n",
    "# plt.legend()\n",
    "# plt.xlabel('date')\n",
    "# plt.ylabel('revenue')\n",
    "# plt.title(\"DAILY REVENUE OF STORE S0084\")\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.margins(0,0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_pivot = df_sales.pivot(index=[\"store_id\",\"date\"], columns=\"product_id\", values=\"revenue\")\n",
    "df_sales_pivot.fillna(0, inplace=True)\n",
    "df_sales_pivot = df_sales_pivot.reset_index().merge(df_sales_daily)\n",
    "df_sales_pivot['total_revenue_trend'] = df_sales_pivot.groupby(\"store_id\")[\"total_revenue\"].transform(lambda series: STL(series, period=period_in_days).fit().trend)\n",
    "df_sales_pivot.set_index([\"store_id\",\"date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_promotion_pivot = df_sales.pivot(index=[\"store_id\",\"date\"], columns=\"product_id\", values=\"promo\")\n",
    "df_promotion_pivot.fillna(False, inplace=True)\n",
    "\n",
    "product_list = list(df_promotion_pivot.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 629/629 [08:27<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "df_heartbeat_flag = pd.DataFrame(index=df_sales_pivot.index)\n",
    "\n",
    "# Season-Trend decomposition using LOESS\n",
    "for product_A in tqdm(product_list):\n",
    "    hearbeat_flag = df_sales_pivot.groupby(level=0)[product_A].transform(lambda series: decompose_signal(series, period_in_days=period_in_days, minimum_heartbeat=0.85)['heartbeat_flag'])\n",
    "    df_heartbeat_flag = pd.concat([df_heartbeat_flag, hearbeat_flag], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 629/629 [11:40<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "df_trend_pivot = pd.DataFrame(index=df_sales_pivot.index)\n",
    "\n",
    "# Season-Trend decomposition using LOESS\n",
    "for product_A in tqdm(product_list):\n",
    "    trend = df_sales_pivot.groupby(level=0)[product_A].transform(lambda series: decompose_signal(series, period_in_days=period_in_days, minimum_heartbeat=0.85)['trend'])\n",
    "    df_trend_pivot = pd.concat([df_trend_pivot, trend], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 629/629 [12:00<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "df_residual_pivot = pd.DataFrame(index=df_sales_pivot.index)\n",
    "\n",
    "# Season-Trend decomposition using LOESS\n",
    "for product_A in tqdm(product_list):\n",
    "    residual = df_sales_pivot.groupby(level=0)[product_A].transform(lambda series: decompose_signal(series, period_in_days=period_in_days, minimum_heartbeat=0.85)['residual'])\n",
    "    df_residual_pivot = pd.concat([df_residual_pivot, residual], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 629/629 [01:17<00:00,  8.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# List of products\n",
    "sku_analysis = []\n",
    "\n",
    "min_promo_days=5\n",
    "min_regular_days=10\n",
    "holidays = pd.Series([]).astype(\"datetime64[ns]\") # assume that there is no holiday effect\n",
    "\n",
    "for product_A in tqdm(product_list):\n",
    "\n",
    "    temp1 = pd.concat([\n",
    "            df_sales_pivot[[product_A]].rename(columns={product_A:\"revenue\"}), \n",
    "            df_promotion_pivot[[product_A]].rename(columns={product_A:\"promo\"}), \n",
    "            df_heartbeat_flag[[product_A]].rename(columns={product_A:\"heartbeat\"})\n",
    "        ], axis=1)\n",
    "    \n",
    "    for store_id, temp2 in temp1.groupby(level=0):\n",
    "        \n",
    "        idx_holiday_to_exclude = temp2.index.get_level_values(1).isin(holidays)\n",
    "        product_behaviour = compare_promo_regular_sales(\n",
    "            sales=temp2[\"revenue\"], \n",
    "            promo=temp2[\"promo\"],\n",
    "            inferred_availability=temp2[\"heartbeat\"],\n",
    "            idx_holiday_to_exclude=idx_holiday_to_exclude,\n",
    "            min_promo_days=min_promo_days, \n",
    "            min_regular_days=min_regular_days\n",
    "        )\n",
    "\n",
    "        if product_behaviour != []:\n",
    "            temp3 = product_behaviour[0]\n",
    "            temp3[\"product_id\"] = product_A\n",
    "            temp3[\"store_id\"] = store_id\n",
    "            sku_analysis.append(temp3)\n",
    "            \n",
    "    \n",
    "# Stick the dicts into a DF\n",
    "df_snap_stats = pd.DataFrame(sku_analysis)\n",
    "df_snap_stats.set_index([\"store_id\",\"product_id\"], inplace=True)\n",
    "# clean the empty dictionaries\n",
    "idx_nonsense = df_snap_stats.num_promo_slots.isna() | (df_snap_stats.num_promo_slots < 1)\n",
    "df_snap_stats = df_snap_stats[~idx_nonsense].copy()\n",
    "\n",
    "# for backwards compatibility\n",
    "df_snap_stats['mu_difference'] = df_snap_stats['difference_averages_promo_to_regular']\n",
    "# Add a small offset to avoid 0-divisions\n",
    "df_snap_stats['mu_delta'] = df_snap_stats.apply(lambda snap_reg: (snap_reg['avg_promo_sales']+0.01)/(snap_reg['avg_regular_sales']+0.01), axis=1)\n",
    "\n",
    "# save the file\n",
    "df_snap_stats.sort_values(by=['mu_delta'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential cannibals\n",
    "# This threshold is used to detect uplifters based on the difference in average sales\n",
    "sales_threshold = 1/3\n",
    "# min AVG sales to be considered\n",
    "min_avg_sales = 2\n",
    "\n",
    "# A bit of work on the uplifters\n",
    "# Snap sales greater than the sales + threshold\n",
    "idx_A = df_snap_stats['mu_delta']>=(1+sales_threshold)\n",
    "\n",
    "# Comparison only valid if they are not zero sales\n",
    "idx_B = (df_snap_stats['avg_promo_sales']>min_avg_sales) & (df_snap_stats['avg_regular_sales']>min_avg_sales)\n",
    "df_snap_stats['uplift_in_median'] = (idx_A & idx_B)\n",
    "\n",
    "# Get two groups: potential cannibals (and haloers) and victims\n",
    "df_snap_uplifters = df_snap_stats[df_snap_stats['uplift_in_median']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1265-S0020-P0316\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 92.31\n",
      ">>> P0189\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 83.92\n",
      ">>> P0260\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 96.20\n",
      ">>> P0436\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 95.10\n",
      ">>> P0639\n",
      "1/1265-S0051-P0305\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 95.80\n",
      ">>> P0428\n",
      "2/1265-S0094-P0559\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 73.03\n",
      ">>> P0332\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 99.00\n",
      ">>> P0436\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 87.61\n",
      ">>> P0459\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 95.50\n",
      ">>> P0560\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 95.20\n",
      ">>> P0569\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 99.80\n",
      ">>> P0663\n",
      "3/1265-S0040-P0390\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 96.60\n",
      ">>> P0182\n",
      "4/1265-S0031-P0439\n",
      "5/1265-S0103-P0681\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 77.32\n",
      ">>> P0046\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 82.52\n",
      ">>> P0283\n",
      "6/1265-S0038-P0730\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 93.61\n",
      ">>> P0655\n",
      "7/1265-S0040-P0340\n",
      "8/1265-S0104-P0340\n",
      "9/1265-S0001-P0316\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 96.80\n",
      ">>> P0054\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 96.40\n",
      ">>> P0079\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 62.44\n",
      ">>> P0103\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 93.91\n",
      ">>> P0212\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 67.23\n",
      ">>> P0333\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 93.91\n",
      ">>> P0561\n",
      "10/1265-S0126-P0316\n",
      "11/1265-S0085-P0340\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 86.51\n",
      ">>> P0130\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 85.91\n",
      ">>> P0140\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 77.22\n",
      ">>> P0187\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 90.11\n",
      ">>> P0227\n",
      "12/1265-S0108-P0340\n",
      "13/1265-S0110-P0559\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 86.91\n",
      ">>> P0219\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 100.00\n",
      ">>> P0261\n",
      "14/1265-S0110-P0439\n",
      "15/1265-S0066-P0305\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 98.90\n",
      ">>> P0035\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 90.31\n",
      ">>> P0325\n",
      "16/1265-S0085-P0102\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 78.82\n",
      ">>> P0005\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 87.31\n",
      ">>> P0122\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 79.32\n",
      ">>> P0130\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 75.72\n",
      ">>> P0187\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 100.00\n",
      ">>> P0439\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 91.41\n",
      ">>> P0491\n",
      "17/1265-S0097-P0130\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 92.61\n",
      ">>> P0140\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 93.31\n",
      ">>> P0280\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 82.72\n",
      ">>> P0611\n",
      "18/1265-S0038-P0679\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 85.11\n",
      ">>> P0046\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 87.31\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 81.32\n",
      ">>> P0129\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 69.73\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 79.12\n",
      ">>> P0171\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 50.55\n",
      ">>> P0353\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 92.11\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 95.30\n",
      ">>> P0390\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 87.31\n",
      ">>> P0408\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 92.31\n",
      ">>> P0461\n",
      "Running Causal Impact...\n",
      "CausalImpact >> Probability of a causal event 94.61\n",
      ">>> P0521\n",
      "Running Causal Impact...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# If true, use the sales without the weekly pattern\n",
    "do_decomposition = False\n",
    "\n",
    "# This is the minimum bump between regular and promo\n",
    "# and promo back to regular *0.25\n",
    "min_diff_in_units_from_reg_to_promo = 2\n",
    "\n",
    "# values for the CI analysis\n",
    "min_ratio_change = 0.4\n",
    "do_exclude_promos_SKU_B = True\n",
    "# This flag is pretty good for debugging/development\n",
    "be_verbose=False\n",
    "\n",
    "# Option to deseasonalise\n",
    "sku_potential_cannibals = df_snap_uplifters.index.tolist()\n",
    "\n",
    "df_CI_analysis = pd.DataFrame()\n",
    "\n",
    "total_cannibals = len(sku_potential_cannibals)\n",
    "\n",
    "for idx, dimension in enumerate(sku_potential_cannibals):\n",
    "\n",
    "    store_id, product_A = dimension\n",
    "    print(f'{idx}/{total_cannibals}-{store_id}-{product_A}')\n",
    "\n",
    "    df_promotion_sub = df_promotion_pivot[df_promotion_pivot.index.get_level_values(0)==store_id].reset_index(level=0)\n",
    "    df_heartbeat_sub = df_heartbeat_flag[df_heartbeat_flag.index.get_level_values(0)==store_id].reset_index(level=0)\n",
    "    df_sales_sub = df_sales_pivot[df_sales_pivot.index.get_level_values(0)==store_id].reset_index(level=0)\n",
    "    df_trend_sub = df_trend_pivot[df_trend_pivot.index.get_level_values(0)==store_id].reset_index(level=0)\n",
    "    df_residual_sub = df_residual_pivot[df_residual_pivot.index.get_level_values(0)==store_id].reset_index(level=0)\n",
    "    idx_holiday_to_exclude = df_sales_sub.index.isin(holidays)\n",
    "\n",
    "    # Get the promotions and split them into slots\n",
    "    promo_sku_A = df_promotion_sub[product_A]\n",
    "    idx_pre_intervention, idx_post_intervention = split_promos_into_sequences(promo_sku_A, min_promo_days=min_promo_days, min_regular_days=min_regular_days)\n",
    "\n",
    "    availability_sku_A = df_heartbeat_sub[product_A]\n",
    "    availability_value_sku_A = availability_sku_A.sum()/len(availability_sku_A)\n",
    "    flag_min_availability_sku_A = availability_value_sku_A > 0.9\n",
    "\n",
    "    # TO-DO: Decomposition should be done according to the SKU's patterns\n",
    "    if flag_min_availability_sku_A & do_decomposition:\n",
    "        sales_sku_A = df_trend_sub[product_A] + df_residual_sub[product_A]\n",
    "    else:\n",
    "        sales_sku_A = df_sales_sub[product_A]\n",
    "\n",
    "\n",
    "    # go through all the SKUs in the store\n",
    "    sku_potential_victims = product_list.copy()\n",
    "    category_A = product_hierarchy[product_hierarchy[\"product_id\"]==product_A][\"hierarchy1_id\"].iloc[0]\n",
    "    sku_potential_victims = product_hierarchy[\n",
    "        (product_hierarchy[\"hierarchy1_id\"]==category_A)& # cannibalization should be only considered when products are in the same category\n",
    "        (product_hierarchy[\"product_id\"]!=product_A)& # product B must be different from product A\n",
    "        (product_hierarchy[\"product_id\"].isin(product_list)) # product B must have revenue\n",
    "    ][\"product_id\"].tolist()\n",
    "\n",
    "    for product_B in sku_potential_victims:\n",
    "        availability_sku_B = df_heartbeat_sub[product_B]\n",
    "        promo_sku_B = df_promotion_sub[product_B]\n",
    "\n",
    "        # Decide what to do with sku_B on promo\n",
    "        # - Remove the promo days? when? outside the cannibalisation window?\n",
    "        # - We should compare windows of pre/post promo, not the entire year\n",
    "        availability_value_sku_B = availability_sku_B.sum()/len(availability_sku_B)\n",
    "        flag_min_availability_sku_B = availability_value_sku_B > 0.9\n",
    "\n",
    "        if flag_min_availability_sku_B & do_decomposition:\n",
    "            sales_sku_B = df_residual_sub[product_B] + df_trend_sub[product_B]\n",
    "        else:\n",
    "            sales_sku_B = df_sales_sub[product_B]\n",
    "        \n",
    "        ci_analysis = calculate_causal_impact_with_covariates(\n",
    "                promo_sku_A=promo_sku_A,\n",
    "                availability_sku_A=availability_sku_A,\n",
    "                sales_sku_B=sales_sku_B, \n",
    "                promo_sku_B=promo_sku_B,\n",
    "                availability_sku_B=availability_sku_B,\n",
    "                idx_pre_intervention=idx_pre_intervention,\n",
    "                idx_post_intervention=idx_post_intervention,\n",
    "                idx_holiday_to_exclude=idx_holiday_to_exclude,\n",
    "                min_diff_in_units_from_reg_to_promo=min_diff_in_units_from_reg_to_promo,\n",
    "                min_ratio_change=min_ratio_change,\n",
    "                do_exclude_promos_SKU_B=do_exclude_promos_SKU_B,\n",
    "                be_verbose=be_verbose,\n",
    "                min_overlapping_days_promo=min_promo_days,\n",
    "                min_overlapping_days_regular=min_regular_days\n",
    "            )\n",
    "        if ci_analysis != []:\n",
    "            print(f\">>> {product_B}\")\n",
    "            causal_impact_analysis = pd.DataFrame(ci_analysis)\n",
    "            causal_impact_analysis[[\"store_id\",\"product_id_A\",\"product_id_B\"]] = store_id, product_A, product_B\n",
    "            df_CI_analysis = pd.concat([df_CI_analysis, causal_impact_analysis])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
