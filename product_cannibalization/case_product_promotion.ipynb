{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_hierarchy = pd.read_csv('/home/tdjiang/github/stored_data/retail_sales_data/product_hierarchy.csv')\n",
    "store_cities = pd.read_csv('/home/tdjiang/github/stored_data/retail_sales_data/store_cities.csv')\n",
    "\n",
    "sales_zip = ZipFile('/home/tdjiang/github/stored_data/retail_sales_data/sales.csv.zip', 'r')\n",
    "sales_data = sales_zip.read('sales.csv')\n",
    "sales_bytes = io.BytesIO(sales_data)\n",
    "sales_bytes.seek(0)\n",
    "sales = pd.read_csv(sales_bytes, converters={'promo_bin_1':str,'promo_bin_2':str,'promo_discount_type_2':str})\n",
    "df = sales.query(\"sales != 0 and ~sales.isnull()\").reset_index(drop=True)\n",
    "df['is_promo'] = df[['promo_bin_1','promo_bin_2']].sum(axis=1) != ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from causalimpact import CausalImpact\n",
    "import networkx as nx\n",
    "\n",
    "def_colours = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "df_products = pd.read_csv('./data/products.csv')\n",
    "df_products.columns = map(lambda x: x.lower(), df_products.columns)\n",
    "df_products = df_products.applymap(lambda x: x.capitalize() if isinstance(x, str) else x)\n",
    "df_products.set_index('product_key', inplace=True)\n",
    "\n",
    "fcn_compare = lambda a,b: abs(a-b)/max(a,b)\n",
    "\n",
    "\n",
    "# Season-Trend decomposition using LOESS.\n",
    "def decompose_signal(input_signal, period_in_days=14, minimum_heartbeat=0.85):\n",
    "    sales_decomposition_LOESS = STL(input_signal, period=period_in_days).fit()\n",
    "    seasonality_flag = sales_decomposition_LOESS.trend > minimum_heartbeat\n",
    "    return pd.DataFrame({'heartbeat_flag': seasonality_flag,\n",
    "            'trend': sales_decomposition_LOESS.trend,\n",
    "            'seasonal': sales_decomposition_LOESS.seasonal,\n",
    "            'residual': sales_decomposition_LOESS.resid})\n",
    "\n",
    "def get_taxonomy_from_sku_name_CFAV(sku_name):\n",
    "    sku_id = int(sku_name.split('_')[0])\n",
    "    category = df_products.at[sku_id,'product_category_name']\n",
    "    group = df_products.at[sku_id,'product_group_name']\n",
    "    return category, group, sku_id\n",
    "\n",
    "\n",
    "def compare_promo_regular_sales(df_store, df_components, sku_A, idx_holiday_to_exclude, min_promo_days=3, min_regular_days=6):\n",
    "    '''\n",
    "        Explain this well as it will go on the paper...\n",
    "        \n",
    "        This method splits a CFAV SKU into promotional and regular chunks taking into account the inferred availability (using LOESS)\\\n",
    "        and the holiday periods (any other event can be included in that idx).\n",
    "        \n",
    "        The method divides the selling days into sequences of regular and normal days and calculates marginal sales.\n",
    "    \n",
    "        'avg_promo_sales' and 'avg_regular_sales' are the sales aggregated across all the slots, whereas\n",
    "        'slot_promo_avg_sales' and 'slot_promo_avg_sales' represent each slot.\n",
    "        \n",
    "        The total sales and total days are not returned, will I need them?\n",
    "        \n",
    "        min_promo_days=3, min_regular_days=6 decide the minimum number of days to be taken into consideration for the sequences.\n",
    "        \n",
    "            TO-DO: Add the beginning and end of the promotional periods\n",
    "        \n",
    "        Updates:\n",
    "        25.10.2020 - First attempt\n",
    "    \n",
    "    '''\n",
    "    _, _, sku_id_A = get_taxonomy_from_sku_name_CFAV(sku_A)\n",
    "\n",
    "    sales_sku_A = df_store[f'{sku_id_A}_sales']\n",
    "    promo_sku_A = df_store[f'{sku_id_A}_is_promotion']\n",
    "    inferred_availability_sku_A = df_components[f'{sku_id_A}_heartbeat_flag']\n",
    "    \n",
    "    analysis_results = []\n",
    "    \n",
    "    # only if there are promos\n",
    "    if promo_sku_A.sum() > 0:\n",
    "\n",
    "        availability_sku_A = inferred_availability_sku_A & (~idx_holiday_to_exclude)\n",
    "        availability_value_sku_A = availability_sku_A.sum()/len(availability_sku_A)\n",
    "\n",
    "        # Split the promotions into slots\n",
    "        idx_pre_intervention, idx_post_intervention = split_promos_into_sequences(promo_sku_A, \\\n",
    "            min_promo_days=min_promo_days, min_regular_days=min_regular_days)\n",
    "\n",
    "        num_promo_slots = len(idx_pre_intervention)\n",
    "\n",
    "        slot_promo_sales = np.zeros(num_promo_slots)\n",
    "        slot_regular_sales = np.zeros(num_promo_slots)\n",
    "\n",
    "        slot_promo_days = np.zeros(num_promo_slots)\n",
    "        slot_regular_days = np.zeros(num_promo_slots)\n",
    "\n",
    "        for idx_promo_slot in range(0, num_promo_slots):\n",
    "            idx_pre_intervention_current = idx_pre_intervention[idx_promo_slot]\n",
    "            idx_post_intervention_current = idx_post_intervention[idx_promo_slot]\n",
    "\n",
    "            slot_promo_sales[idx_promo_slot] = sales_sku_A[idx_post_intervention_current].sum()\n",
    "            slot_promo_days[idx_promo_slot] = idx_post_intervention_current.sum()\n",
    "\n",
    "            slot_regular_sales[idx_promo_slot] = sales_sku_A[idx_pre_intervention_current].sum()\n",
    "            slot_regular_days[idx_promo_slot] = idx_pre_intervention_current.sum()\n",
    "\n",
    "        slot_promo_avg_sales = np.divide(slot_promo_sales, slot_promo_days)\n",
    "        slot_regular_avg_sales = np.divide(slot_regular_sales, slot_regular_days)\n",
    "        # totals\n",
    "        total_slot_promo_days = slot_promo_days.sum()\n",
    "        if total_slot_promo_days>0:\n",
    "            avg_promo_sales = slot_promo_sales.sum()/total_slot_promo_days\n",
    "        else:\n",
    "            avg_promo_sales = 0\n",
    "        \n",
    "        total_slot_regular_days = slot_regular_days.sum()\n",
    "        if total_slot_regular_days>0:\n",
    "            avg_regular_sales = slot_regular_sales.sum()/total_slot_regular_days\n",
    "        else:\n",
    "            avg_regular_sales = 0\n",
    "            \n",
    "\n",
    "        # difference between the averages during promo and regular\n",
    "        difference_averages_promo_to_regular = avg_promo_sales-avg_regular_sales\n",
    "        # cumulative difference\n",
    "        cum_difference_sales_promo_to_regular = slot_promo_sales.sum()-slot_regular_sales.sum()\n",
    "        \n",
    "        analysis_results.append({'num_promo_slots': num_promo_slots,\n",
    "        'avg_promo_sales': avg_promo_sales,\n",
    "        'avg_regular_sales': avg_regular_sales,\n",
    "        'promo_days': total_slot_promo_days, \n",
    "        'regular_days':total_slot_regular_days,\n",
    "        'difference_averages_promo_to_regular': difference_averages_promo_to_regular,\n",
    "        'cum_difference_sales_promo_to_regular': cum_difference_sales_promo_to_regular,\n",
    "        'slot_promo_avg_sales': slot_promo_avg_sales,\n",
    "        'slot_regular_avg_sales': slot_regular_avg_sales,\n",
    "        'availability_value_sku_A': availability_value_sku_A})\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "def holiday_to_exclude(df_summary:'pd.DataFrame', holidays):\n",
    "    idx_holidays  = df_summary.index.isin(holidays)\n",
    "    idx_covid19 = (df_summary.index >= 20210701) & (df_summary.index <= 20211001)\n",
    "    return idx_holidays | idx_covid19\n",
    "\n",
    "def max_first_sales_date_shop_AB(df_shops:'pd.DataFrame', shop_code_A, shop_code_B):\n",
    "    return int(df_shops.query(f'shop_code.isin({[shop_code_A,shop_code_B]})').min_date.max().replace('-',''))\n",
    "\n",
    "def customers_history_shop(shop_code, folder='./data/customers_per_shop/hcm'):\n",
    "    df = pd.read_csv(f'{folder}/{shop_code}.csv')\n",
    "    return df\n",
    "\n",
    "def customers_by_date_shop_pairs(customers_history_AB:'pd.DataFrame', period_in_days=7):\n",
    "    df = customers_history_AB\\\n",
    "        .groupby(['date_key','shop_code'], as_index=False)\\\n",
    "        .agg(\n",
    "            customers=('customer_key','nunique')\n",
    "        )\\\n",
    "        .pivot(index='date_key',columns='shop_code',values='customers')\\\n",
    "        .fillna(0)\\\n",
    "        .applymap(int)\n",
    "    return df\n",
    "\n",
    "def customers_cross_from_victim_shop(customers_history_AB:'pd.DataFrame'):\n",
    "\n",
    "    shop_code_A, shop_code_B = customers_history_AB.shop_code.unique()\n",
    "\n",
    "    df = customers_history_AB\\\n",
    "        .assign(\n",
    "            is_cross_customer = lambda s: (s.date_key > s.groupby('customer_key')['date_key'].transform('min')) & (s.date_key == s.groupby(['customer_key','shop_key'])['date_key'].transform('min'))\n",
    "        )\\\n",
    "        .query(f\"is_cross_customer\")\\\n",
    "        .groupby(['date_key','shop_code'], as_index=False)\\\n",
    "        .agg(\n",
    "            cross_customers=('customer_key','nunique')\n",
    "        )\\\n",
    "        .pivot(index='date_key', columns='shop_code', values='cross_customers')\\\n",
    "        .rename(columns={shop_code_A:f'{shop_code_A}_cross',shop_code_B:f'{shop_code_B}_cross'})\\\n",
    "        .fillna(0)\\\n",
    "        .applymap(int)\n",
    "    \n",
    "    if f'{shop_code_A}_cross' not in df.columns:\n",
    "        df[f'{shop_code_A}_cross'] = 0\n",
    "    \n",
    "    if f'{shop_code_B}_cross' not in df.columns:\n",
    "        df[f'{shop_code_B}_cross'] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def customers_component_AB(decompostition):\n",
    "    shop_code_A, shop_code_B = [col for col in customers_by_date_shop_pairs.columns if isinstance(col, int)]\n",
    "    customers_shop_A = customers_by_date_shop_pairs[shop_code_A]\n",
    "    df_decomposition_A = decompose_signal()\n",
    "\n",
    "\n",
    "\n",
    "def compare_cross_regular_customers(df_store:'pd.DataFrame', df_history:'pd.DataFrame', df_components:'pd.DataFrame', shop_code_A, shop_code_B, min_date_key, holidays, min_cross_days=5, min_regular_days=10):\n",
    "\n",
    "    df_cross = df_history\\\n",
    "        .query(f'shop_code.isin({[shop_code_A,shop_code_B]})')\\\n",
    "        .assign(\n",
    "            is_cross_customer = lambda s: (s.date_key > s.groupby('customer_key')['date_key'].transform('min')) & (s.date_key == s.groupby(['customer_key','shop_key'])['date_key'].transform('min'))\n",
    "        )\\\n",
    "        .query(f\"is_cross_customer\")\\\n",
    "        .groupby(['date_key','shop_code'], as_index=False)\\\n",
    "        .agg(\n",
    "            cross_customers=('customer_key','nunique')\n",
    "        )\\\n",
    "        .pivot(index='date_key', columns='shop_code', values='cross_customers')\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={shop_code_A:f'{shop_code_A}_cross',shop_code_B:f'{shop_code_B}_cross'})\\\n",
    "        .fillna(0)\\\n",
    "        .applymap(int)\n",
    "    \n",
    "    df_summary = df_store[['date_key',shop_code_A,shop_code_B]]\\\n",
    "        .merge(df_cross, on='date_key', how='left')\\\n",
    "        .fillna(0)\\\n",
    "        .applymap(int)\\\n",
    "        .query(f'date_key >= {min_date_key}')\n",
    "    df_summary[[f'{shop_code_A}_iscross',f'{shop_code_B}_iscross']] = df_summary[[f'{shop_code_A}_cross',f'{shop_code_B}_cross']].applymap(lambda x: False if x==0 else True)\n",
    "\n",
    "    customers_shop_A = df_summary[shop_code_A]\n",
    "    cross_shop_A = df_summary[f'{shop_code_A}_iscross']\n",
    "    inferred_availability_shop_A = df_components.query(f'date_key >= {min_date_key}')[f'{shop_code_A}_heartbeat_flag']\n",
    "    \n",
    "    analysis_results = []\n",
    "    idx_holiday_to_exclude = holiday_to_exclude(df_summary, holidays)\n",
    "    \n",
    "    # only if there are cross shops\n",
    "    if cross_shop_A.sum() > 0:\n",
    "\n",
    "        availability_shop_A = inferred_availability_shop_A & (~idx_holiday_to_exclude)\n",
    "        availability_value_shop_A = availability_shop_A.sum()/len(availability_shop_A)\n",
    "\n",
    "        # Split the cross shops into slots\n",
    "        idx_pre_intervention, idx_post_intervention = split_cross_into_sequences(cross_shop_A, \\\n",
    "            min_cross_days=min_cross_days, min_regular_days=min_regular_days)\n",
    "\n",
    "        num_cross_slots = len(idx_pre_intervention)\n",
    "\n",
    "        slot_cross_customers = np.zeros(num_cross_slots)\n",
    "        slot_regular_customers = np.zeros(num_cross_slots)\n",
    "\n",
    "        slot_cross_days = np.zeros(num_cross_slots)\n",
    "        slot_regular_days = np.zeros(num_cross_slots)\n",
    "\n",
    "        for idx_cross_slot in range(0, num_cross_slots):\n",
    "            idx_pre_intervention_current = idx_pre_intervention[idx_cross_slot]\n",
    "            idx_post_intervention_current = idx_post_intervention[idx_cross_slot]\n",
    "\n",
    "            slot_cross_customers[idx_cross_slot] = customers_shop_A[idx_post_intervention_current].sum()\n",
    "            slot_cross_days[idx_cross_slot] = idx_post_intervention_current.sum()\n",
    "\n",
    "            slot_regular_customers[idx_cross_slot] = customers_shop_A[idx_pre_intervention_current].sum()\n",
    "            slot_regular_days[idx_cross_slot] = idx_pre_intervention_current.sum()\n",
    "\n",
    "        slot_cross_avg_customers = np.divide(slot_cross_customers, slot_cross_days)\n",
    "        slot_regular_avg_customers = np.divide(slot_regular_customers, slot_regular_days)\n",
    "        # totals\n",
    "        total_slot_cross_days = slot_cross_days.sum()\n",
    "        if total_slot_cross_days>0:\n",
    "            avg_cross_customers = slot_cross_customers.sum()/total_slot_cross_days\n",
    "        else:\n",
    "            avg_cross_customers = 0\n",
    "        \n",
    "        total_slot_regular_days = slot_regular_days.sum()\n",
    "        if total_slot_regular_days>0:\n",
    "            avg_regular_customers = slot_regular_customers.sum()/total_slot_regular_days\n",
    "        else:\n",
    "            avg_regular_customers = 0\n",
    "            \n",
    "\n",
    "        # difference between the averages during cross shops and regular\n",
    "        difference_averages_cross_to_regular = avg_cross_customers-avg_regular_customers\n",
    "        # cumulative difference\n",
    "        cum_difference_customers_cross_to_regular = slot_cross_customers.sum()-slot_regular_customers.sum()\n",
    "        \n",
    "        analysis_results.append({\n",
    "            'shop_code_A':shop_code_A,\n",
    "            'shop_code_B':shop_code_B,\n",
    "            'num_cross_slots': num_cross_slots,\n",
    "            'avg_cross_customers': avg_cross_customers,\n",
    "            'avg_regular_customers': avg_regular_customers,\n",
    "            'cross_days': total_slot_cross_days, \n",
    "            'regular_days':total_slot_regular_days,\n",
    "            'difference_averages_cross_to_regular': difference_averages_cross_to_regular,\n",
    "            'cum_difference_customers_cross_to_regular': cum_difference_customers_cross_to_regular,\n",
    "            'slot_promo_avg_customers': slot_cross_avg_customers,\n",
    "            'slot_regular_avg_customers': slot_regular_avg_customers,\n",
    "            'availability_value_shop_A': availability_value_shop_A\n",
    "        })\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "\n",
    "def calculate_causal_impact_with_covariates(sku_id_A, promo_sku_A, availability_sku_A, sales_sku_A, \\\n",
    "    sku_id_B, promo_sku_B, availability_sku_B, df_sales_covariates, \\\n",
    "    idx_pre_intervention, idx_post_intervention, \\\n",
    "    idx_holiday_to_exclude,\n",
    "    min_diff_in_units_from_reg_to_promo, \n",
    "    min_ratio_change = 0.3,\n",
    "    do_exclude_promos_SKU_B = True, be_verbose=True, \\\n",
    "    min_overlapping_days_regular=5, min_overlapping_days_promo=3):\n",
    "    '''\n",
    "        This is the method used to populate the paper results\n",
    "    '''\n",
    "\n",
    "    # use this flag to exclude sku_B if on promo\n",
    "    total_days = promo_sku_A.shape[0]\n",
    "    num_days = np.arange(total_days)\n",
    "    combined_availability = availability_sku_A & availability_sku_B & (~idx_holiday_to_exclude)\n",
    "\n",
    "    causal_analysis = []\n",
    "    \n",
    "    sales_sku_B = df_sales_covariates.iloc[:,0]\n",
    "    \n",
    "    total_slots = len(idx_pre_intervention)\n",
    "\n",
    "    for idx_promo_slot in range(0, total_slots):\n",
    "\n",
    "        idx_pre_intervention_current = idx_pre_intervention[idx_promo_slot]\n",
    "        idx_post_intervention_current = idx_post_intervention[idx_promo_slot]\n",
    "\n",
    "        # # #\n",
    "        # promo days == 'post-intervention'\n",
    "        # # #\n",
    "        idx_overlapping_days_promo = combined_availability & idx_post_intervention_current\n",
    "        total_overlapping_days_promo = idx_overlapping_days_promo.sum()\n",
    "\n",
    "\n",
    "        # overlapping promo days. Both SKUs on promo, \"competing promos\"\n",
    "        idx_competing_promo_days = idx_overlapping_days_promo & promo_sku_B\n",
    "        competing_promo_days = idx_competing_promo_days.sum()\n",
    "\n",
    "\n",
    "        # # #\n",
    "        # regular days == 'pre-intervention'\n",
    "        # # #\n",
    "        # A period should not be marked as 'regular' if SKU_B is on promotion\n",
    "        if do_exclude_promos_SKU_B:\n",
    "            idx_overlapping_days_regular = combined_availability & idx_pre_intervention_current & (~promo_sku_B)\n",
    "        else:\n",
    "            idx_overlapping_days_regular = combined_availability & idx_pre_intervention_current\n",
    "        total_overlapping_days_regular = idx_overlapping_days_regular.sum()\n",
    "        \n",
    "        # Minimum requirements of overlap. Otherwise the analysis does not make much sense.\n",
    "        # numerical index (for Causal Impact)\n",
    "        if (total_overlapping_days_regular>=min_overlapping_days_regular) & \\\n",
    "            (total_overlapping_days_promo>=min_overlapping_days_promo):\n",
    "            \n",
    "            ind_regular_days = num_days[idx_overlapping_days_regular]\n",
    "            start_regular = ind_regular_days.min()\n",
    "            end_regular = ind_regular_days.max()\n",
    "            \n",
    "            ind_promo_days = num_days[idx_overlapping_days_promo]\n",
    "            start_promo = ind_promo_days.min()\n",
    "            end_promo = ind_promo_days.max()\n",
    "\n",
    "            # sales of SKU_B\n",
    "            # during regular\n",
    "            sku_B_regular_avg_sales = sales_sku_B[idx_overlapping_days_regular].mean()\n",
    "            # during promo\n",
    "            sku_B_avg_sales_during_promo_sku_A = sales_sku_B[idx_overlapping_days_promo].mean()\n",
    "            # Difference in average sales between the regular and the promotional one.\n",
    "            diff_in_units_from_reg_to_promo = sku_B_regular_avg_sales-sku_B_avg_sales_during_promo_sku_A\n",
    "            \n",
    "            # can we review the post promotional sales?\n",
    "            post_period_start = end_promo+1\n",
    "            post_period_end = np.min([end_promo+8, total_days])\n",
    "            post_promo_days = post_period_end-post_period_start+1\n",
    "            sku_B_regular_post_promo_avg_sales = sales_sku_B[post_period_start:post_period_end].mean()\n",
    "            # post promo should be larger than during the cannibalisation\n",
    "            diff_in_units_from_promo_to_pos_promo = sku_B_avg_sales_during_promo_sku_A-sku_B_regular_post_promo_avg_sales\n",
    "            \n",
    "            post_promo_flag = (-diff_in_units_from_promo_to_pos_promo > min_diff_in_units_from_reg_to_promo*0.25)\n",
    "            \n",
    "            '''\n",
    "            if idx_promo_slot+1 < total_slots:\n",
    "                # during regular\n",
    "                idx_reg_post_intervention = idx_pre_intervention[idx_promo_slot+1]\n",
    "                sku_B_regular_post_promo_avg_sales = sales_sku_B[idx_reg_post_intervention].mean()\n",
    "                # post promo should be larger than during the cannibalisation\n",
    "                diff_in_units_from_promo_to_pos_promo = sku_B_avg_sales_during_promo_sku_A-sku_B_regular_post_promo_avg_sales\n",
    "                post_promo_flag = (-diff_in_units_from_promo_to_pos_promo>min_diff_in_units_from_reg_to_promo)\n",
    "            else:\n",
    "                diff_in_units_from_promo_to_pos_promo = np.nan\n",
    "                post_promo_flag = True\n",
    "            '''\n",
    "\n",
    "            # delta\n",
    "            ratio_change = fcn_compare(sku_B_avg_sales_during_promo_sku_A, sku_B_regular_avg_sales)\n",
    "            if be_verbose:\n",
    "                print(f'Summary of the current scenario (slot {idx_promo_slot})')\n",
    "                print(f'Before SKU A going on promo, the SKUs overlap for {total_overlapping_days_regular} days (promos on sku_B excluded: {do_exclude_promos_SKU_B})')\n",
    "                if end_regular+1<start_promo:\n",
    "                    print(f'There is a gap of {start_promo-end_regular} days between the regular days and the beginning of the promotion (due to availability/promotional period of SKU_B)')\n",
    "                print(f'When SKU_A is on promo, the SKUs overlap for {total_overlapping_days_promo} days')\n",
    "                print(f'During the overlapping days, SKU B is on promo for {competing_promo_days} days')\n",
    "\n",
    "                print(f'Average sales of sku B before sku A on promo {sku_B_regular_avg_sales:.2f}')\n",
    "                print(f'Average sales of sku B during sku A on promo {sku_B_avg_sales_during_promo_sku_A:.2f}')\n",
    "                print(f'Average sales of sku B after sku A on promo {sku_B_regular_post_promo_avg_sales:.2f} over {post_promo_days} days - {post_promo_flag}')\n",
    "                print(f'Diff in units from regular to promotion {-diff_in_units_from_reg_to_promo:.2f}')\n",
    "                print(f'Diff in units from cannibalisation to regular {-diff_in_units_from_promo_to_pos_promo:.2f}')\n",
    "                \n",
    "                print(f'Ratio of change {ratio_change:3.2f} (the lower the closer (0,1)) {ratio_change>min_ratio_change} \\n')\n",
    "\n",
    "\n",
    "            if (ratio_change>min_ratio_change) & (diff_in_units_from_reg_to_promo > min_diff_in_units_from_reg_to_promo) & post_promo_flag:\n",
    "\n",
    "                idx_regular_days = np.array([start_regular, end_regular]).tolist()\n",
    "                idx_promo_days   = np.array([start_promo, end_promo]).tolist()\n",
    "\n",
    "                print('Running Causal Impact...')\n",
    "                ci = CausalImpact(df_sales_covariates, idx_regular_days, idx_promo_days)\n",
    "                '''\n",
    "                https://github.com/dafiti/causalimpact/blob/8d881fc5c270348d8c8ff59c936997a75d7c5fac/causalimpact/main.py#L88\n",
    "                \n",
    "                First column must contain the `y` measured value \n",
    "                while the others contain the covariates `X` that are used in the \n",
    "                linear regression component of the model.\n",
    "                '''\n",
    "                #ci.lower_upper_percentile\n",
    "                avg_actual = ci.summary_data.loc['actual', 'average']\n",
    "                # Had the promo not been launched on the cannibal, we would have sold this amount.\n",
    "                avg_predicted = ci.summary_data.loc['predicted', 'average']\n",
    "                avg_abs_effect = ci.summary_data.loc['abs_effect', 'average']\n",
    "                # This can be seen as the number of units that the cannibal is taking from the victim\n",
    "                cum_abs_effect = ci.summary_data.loc['abs_effect', 'cumulative']\n",
    "                posterior_tail_prob = ci.p_value\n",
    "                prob_causal_effect = (1-ci.p_value)*100\n",
    "                print(f'CausalImpact >> Probability of a causal event {prob_causal_effect:.2f}')\n",
    "\n",
    "                temp_dict = {'cannibal':sku_id_A,\n",
    "                            'victim':sku_id_B,\n",
    "                            'slot_number': idx_promo_slot,\n",
    "                            'idx_regular_days': idx_regular_days,\n",
    "                            'idx_promo_days': idx_promo_days,\n",
    "                            'total_overlapping_days_regular': total_overlapping_days_regular,\n",
    "                            'regular_to_promo_gap': start_promo-end_regular-1,\n",
    "                            'total_overlapping_days_promo': total_overlapping_days_promo,\n",
    "                            'competing_promo_days': competing_promo_days,\n",
    "                            'sku_B_regular_avg_sales': sku_B_regular_avg_sales,\n",
    "                            'sku_B_avg_sales_during_promo_sku_A': sku_B_avg_sales_during_promo_sku_A,\n",
    "                            'diff_in_units_from_reg_to_promo': diff_in_units_from_reg_to_promo,\n",
    "                            'diff_in_units_from_promo_to_pos_promo': diff_in_units_from_promo_to_pos_promo,\n",
    "                            'ratio_change': ratio_change,\n",
    "                            'avg_actual':avg_actual,\n",
    "                            'avg_predicted': avg_predicted,\n",
    "                            'avg_abs_effect': avg_abs_effect,\n",
    "                            'cum_abs_effect': cum_abs_effect,\n",
    "                            'posterior_tail_prob': posterior_tail_prob,\n",
    "                            'prob_causal_effect': prob_causal_effect\n",
    "                            }\n",
    "                causal_analysis.append(temp_dict)\n",
    "                \n",
    "    return causal_analysis\n",
    "\n",
    "def add_graph_relationship(node_A, node_B, edge_properties: dict):\n",
    "    DG = nx.DiGraph()\n",
    "\n",
    "    DG.add_node(node_A['name'], **node_A['properties'])\n",
    "\n",
    "    d = dict()\n",
    "    DG.add_node(node_B['name'], **node_B['properties'])\n",
    "\n",
    "    edge_label = '\\n'.join([f'{k}: {v:3.2f}' for k,v in edge_properties.items()])  \n",
    "    DG.add_edge(node_A['name'], node_B['name'], **edge_properties, label=edge_label)\n",
    "\n",
    "        \n",
    "def split_promos_into_sequences(idx_promos: 'pd.Series', min_promo_days=4, min_regular_days=6):\n",
    "    '''\n",
    "    Group the indices of a promotion into sequences of pre and post promotion\n",
    "    '''\n",
    "    # Groups/sequences\n",
    "    seqs = (idx_promos.shift(1)!=idx_promos).cumsum()\n",
    "    promo_seqs = seqs[idx_promos]\n",
    "    # Indices\n",
    "    idx_pre_intervention = []\n",
    "    idx_post_intervention = []\n",
    "    for value_promo_seqs in promo_seqs.unique():\n",
    "        idx_current_promo = seqs==value_promo_seqs\n",
    "        prev_seq = value_promo_seqs-1\n",
    "        idx_current_regular = seqs==prev_seq\n",
    "        current_promo_length = idx_current_promo.sum()\n",
    "        current_regular_length = idx_current_regular.sum()\n",
    "        if (current_promo_length >= min_promo_days) and (current_regular_length >= min_regular_days):\n",
    "            idx_pre_intervention.append(idx_current_regular)\n",
    "            idx_post_intervention.append(idx_current_promo)\n",
    "    return idx_pre_intervention, idx_post_intervention\n",
    "\n",
    "def split_cross_into_sequences(idx_cross: 'pd.Series', min_cross_days=5, min_regular_days=10):\n",
    "    # Groups/sequences\n",
    "    seqs = (idx_cross.shift(1)!=idx_cross).cumsum()\n",
    "    promo_seqs = seqs[idx_cross]\n",
    "    # Indices\n",
    "    idx_pre_intervention = []\n",
    "    idx_post_intervention = []\n",
    "    for value_promo_seqs in promo_seqs.unique():\n",
    "        idx_current_promo = seqs==value_promo_seqs\n",
    "        prev_seq = value_promo_seqs-1\n",
    "        idx_current_regular = seqs==prev_seq\n",
    "        current_promo_length = idx_current_promo.sum()\n",
    "        current_regular_length = idx_current_regular.sum()\n",
    "        if (current_promo_length >= min_cross_days) and (current_regular_length >= min_regular_days):\n",
    "            idx_pre_intervention.append(idx_current_regular)\n",
    "            idx_post_intervention.append(idx_current_promo)\n",
    "    return idx_pre_intervention, idx_post_intervention\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
