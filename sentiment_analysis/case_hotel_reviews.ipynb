{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### references\n",
    "- aspect based sentiment analysis: https://github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 09:38:49.196548: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-09-07 09:38:49.196594: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-09-07 09:38:51.821239: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-09-07 09:38:51.821293: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-09-07 09:38:51.821317: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (CID-GiangTD13): /proc/driver/nvidia/version does not exist\n",
      "2023-09-07 09:38:51.821687: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some layers from the model checkpoint at absa/classifier-rest-0.2 were not used when initializing BertABSClassifier: ['dropout_379']\n",
      "- This IS expected if you are initializing BertABSClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertABSClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of BertABSClassifier were not initialized from the model checkpoint at absa/classifier-rest-0.2 and are newly initialized: ['dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from utils import data_scraping, absa_english_text\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read & transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "file_path = 'https://raw.githubusercontent.com/dinhgiangltk/stored_data/main/text_data/the_grace_dalat_reviews.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path, converters={'profileId':str})\n",
    "    eval_cols = ['travelPurpose','travelKeywords','photoDataDisplaysList','reactionSummaries']\n",
    "    df[eval_cols] = df[eval_cols].applymap(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "except:\n",
    "    data = data_scraping(url_hotel=\"https://www.traveloka.com/vi-vn/hotel/vietnam/the-grace-hotel-dalat-3000010042556\", reviews_per_page=10)\n",
    "    df = data.get_all_reviews()\n",
    "    df.to_csv('/home/tdjiang/github/stored_data/text_data/the_grace_dalat_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty data to blank data\n",
    "df = df.mask(df == '')\n",
    "\n",
    "# drop columns with no data\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# drop rows with no comment\n",
    "df = df.dropna(subset=['originalReviewText','reviewText'], how='all', axis=0)\n",
    "\n",
    "# add new columns\n",
    "df['travelPurposeText'] = df.travelPurpose.apply(lambda x: x['travelPurposeText'] if isinstance(x, dict) else x)\n",
    "df['travelPurpose'] = df.travelPurpose.apply(lambda x: x['travelPurpose'] if isinstance(x, dict) else x)\n",
    "df['travelKeywords'] = df.travelKeywords.apply(lambda x: ','.join(sorted(map(lambda y: y['travelKeyword'] if isinstance(y, dict) else '', x))) if isinstance(x, list) else x)\n",
    "df['reviewLikes'] = df.reactionSummaries.apply(lambda x: x['reactionSummaryMap']['LIKE']['reactionCount'] if isinstance(x, dict) else x)\n",
    "df['photoCategories'] = df.photoDataDisplaysList.apply(lambda x: ','.join(sorted(map(lambda y: y['photoCategoryDisplay']['photoCategory'] if isinstance(y, dict) else '', x))) if isinstance(x, list) else x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aspect-based sentiment analysis\n",
    "- prioritize the english review text, if none, do the translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0\n",
      "--- 1\n",
      "--- 2\n",
      "--- 3\n",
      "--- 4\n",
      "--- 5\n",
      "--- 6\n",
      "--- 7\n",
      "--- 8\n",
      "--- 9\n",
      "--- 10\n",
      "--- 11\n",
      "--- 12\n",
      "--- 13\n",
      "--- 14\n",
      "--- 15\n",
      "--- 16\n",
      "--- 17\n",
      "--- 18\n",
      "--- 19\n",
      "--- 20\n",
      "--- 21\n",
      "--- 22\n",
      "--- 23\n",
      "--- 24\n",
      "--- 25\n",
      "--- 26\n",
      "--- 27\n",
      "--- 28\n",
      "--- 29\n",
      "--- 30\n",
      "--- 31\n",
      "--- 32\n",
      "--- 33\n",
      "--- 34\n",
      "--- 35\n",
      "--- 36\n",
      "--- 37\n",
      "--- 38\n",
      "--- 39\n",
      "--- 40\n",
      "--- 41\n",
      "--- 42\n",
      "--- 43\n",
      "--- 44\n",
      "--- 45\n",
      "--- 46\n",
      "--- 47\n",
      "--- 48\n",
      "--- 49\n",
      "--- 50\n",
      "--- 51\n",
      "--- 52\n",
      "--- 53\n",
      "--- 54\n",
      "--- 55\n",
      "--- 56\n",
      "--- 57\n",
      "--- 58\n",
      "--- 59\n",
      "--- 60\n",
      "--- 61\n",
      "--- 62\n",
      "--- 63\n",
      "--- 64\n",
      "--- 65\n",
      "--- 66\n",
      "--- 67\n",
      "--- 68\n",
      "--- 69\n",
      "--- 70\n",
      "--- 71\n",
      "--- 72\n",
      "--- 73\n",
      "--- 74\n",
      "--- 75\n",
      "--- 76\n",
      "--- 77\n",
      "--- 78\n",
      "--- 79\n",
      "--- 80\n",
      "--- 81\n",
      "--- 82\n",
      "--- 83\n",
      "--- 84\n",
      "--- 85\n",
      "--- 86\n",
      "--- 87\n",
      "--- 88\n",
      "--- 89\n",
      "--- 90\n",
      "--- 91\n",
      "--- 92\n",
      "--- 93\n",
      "--- 94\n",
      "--- 95\n",
      "--- 96\n",
      "--- 97\n",
      "--- 98\n",
      "--- 99\n",
      "--- 100\n",
      "--- 101\n",
      "--- 102\n",
      "--- 103\n",
      "--- 104\n",
      "--- 105\n",
      "--- 106\n",
      "--- 107\n",
      "--- 108\n",
      "--- 109\n",
      "--- 110\n",
      "--- 111\n",
      "--- 112\n",
      "--- 113\n",
      "--- 114\n",
      "--- 115\n",
      "--- 116\n",
      "--- 117\n",
      "--- 118\n",
      "--- 119\n",
      "--- 120\n",
      "--- 121\n",
      "--- 122\n",
      "--- 123\n",
      "--- 124\n",
      "--- 125\n",
      "--- 126\n",
      "--- 127\n",
      "--- 128\n",
      "--- 129\n",
      "--- 130\n",
      "--- 131\n",
      "--- 132\n",
      "--- 133\n",
      "--- 134\n",
      "--- 135\n",
      "--- 136\n",
      "--- 137\n",
      "--- 138\n",
      "--- 139\n",
      "--- 140\n",
      "--- 141\n",
      "--- 142\n",
      "--- 143\n",
      "--- 144\n",
      "--- 145\n",
      "--- 146\n",
      "--- 147\n",
      "--- 148\n",
      "--- 149\n",
      "--- 150\n",
      "--- 151\n",
      "--- 152\n",
      "--- 153\n",
      "--- 154\n",
      "--- 155\n",
      "--- 156\n",
      "--- 157\n",
      "--- 158\n",
      "--- 159\n",
      "--- 160\n",
      "--- 161\n",
      "--- 162\n",
      "--- 163\n",
      "--- 164\n",
      "--- 165\n",
      "--- 166\n",
      "--- 167\n",
      "--- 168\n",
      "--- 169\n",
      "--- 170\n",
      "--- 171\n",
      "--- 172\n",
      "--- 173\n",
      "--- 174\n",
      "--- 175\n",
      "--- 176\n",
      "--- 177\n",
      "--- 178\n",
      "--- 179\n",
      "--- 180\n",
      "--- 181\n",
      "--- 182\n",
      "--- 183\n",
      "--- 184\n",
      "--- 185\n",
      "--- 186\n",
      "--- 187\n",
      "--- 188\n",
      "--- 189\n",
      "--- 190\n",
      "--- 191\n",
      "--- 192\n",
      "--- 193\n",
      "--- 194\n",
      "--- 195\n",
      "--- 196\n",
      "--- 197\n",
      "--- 198\n",
      "--- 199\n",
      "--- 200\n",
      "--- 201\n",
      "--- 202\n",
      "--- 203\n",
      "--- 204\n",
      "--- 205\n",
      "--- 206\n",
      "--- 207\n",
      "--- 208\n",
      "--- 209\n",
      "--- 210\n",
      "--- 211\n",
      "--- 212\n",
      "--- 213\n",
      "--- 214\n",
      "--- 215\n",
      "--- 216\n",
      "--- 217\n",
      "--- 218\n",
      "--- 219\n",
      "--- 220\n",
      "--- 221\n",
      "--- 222\n",
      "--- 223\n",
      "--- 224\n",
      "--- 225\n",
      "--- 226\n",
      "--- 227\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "\n",
    "file_name = 'output_the_grace_dalat_reviews.csv'\n",
    "if os.path.exists(file_name):\n",
    "    df_output = pd.read_csv(file_name)\n",
    "\n",
    "else:\n",
    "    for id, row in enumerate(df.to_dict(orient='records')):\n",
    "        reviewTextFn = row['reviewText']\n",
    "        absa_class = absa_english_text(reviewTextFn)\n",
    "        if not row['translated']:\n",
    "            tokenized = absa_class.words_tokenized()\n",
    "            reviewTextFn = absa_class.translate_vi_to_en(tokenized)\n",
    "        \n",
    "        # limit first 400 characters\n",
    "        reviewTextFn = absa_class.truncate_first_words(reviewTextFn)\n",
    "        sentiments = absa_class.absa_by_np(reviewTextFn)\n",
    "        output.append({\n",
    "            'reviewId': row['reviewId'],\n",
    "            'sentiment':sentiments\n",
    "        })\n",
    "        print('---', id)\n",
    "\n",
    "    df_output = pd.DataFrame(output)\n",
    "    df_output['sentiment'] = df_output.apply(lambda x: pd.DataFrame(x.sentiment).assign(reviewId = x.reviewId) , axis=1)\n",
    "    df_output = pd.concat(df_output.sentiment.tolist())\n",
    "    df_output.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "room             145\n",
       "hotel            127\n",
       "staff             85\n",
       "center            44\n",
       "market            42\n",
       "price             35\n",
       "time              33\n",
       "service           26\n",
       "location          25\n",
       "bit               23\n",
       "night             22\n",
       "receptionist      19\n",
       "car               19\n",
       "soundproofing     19\n",
       "water             17\n",
       "guest             16\n",
       "point             15\n",
       "window            15\n",
       "view              14\n",
       "day               14\n",
       "Name: aspect, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output.aspect.value_counts().head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
